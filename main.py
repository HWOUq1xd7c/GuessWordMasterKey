import heapq
import faiss
import pickle
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, util
import random

from api import GuessWordAPI


# åˆ¤é¢˜å™¨ç±»ä¿æŒä¸å˜
class LocalJudge:
    def __init__(self, target_word, model, vocab):
        self.target_word = target_word
        self.model = model
        self.vocab = vocab
        if target_word not in vocab:
            raise ValueError("Target word not in vocabulary")
        self.target_embedding = model.encode([target_word], convert_to_tensor=True)

    def query(self, word):
        """ä¾›æœç´¢å™¨è°ƒç”¨çš„ç›¸ä¼¼åº¦è®¡ç®—æ¥å£"""
        if word == self.target_word:
            return 1.0
        if word not in self.vocab:
            return 0.0
        query_embedding = self.model.encode([word], convert_to_tensor=True)
        return util.cos_sim(self.target_embedding, query_embedding).item()

class OnlineJudge:
    def __init__(self):
        try:
            self.api = GuessWordAPI(0.5, 0.1, True)
        except Exception as e:
            print(f"Warning: Could not initialize OnlineJudge API: {e}")
            self.api = None

    def query(self, word):
        if self.api is None:
            print("Error: API not initialized. Cannot query.")
            return 0.0

        result = self.api.guess_word(word)
        if result['correct']:
            return 1.0
        elif result['similarity'] is None:
            return 0.0
        return result['similarity']


class Searcher:
    """è¯è¯­æœç´¢å™¨ç±»ï¼Œè´Ÿè´£æ‰§è¡Œæœç´¢ç­–ç•¥"""

    def __init__(self, faiss_index, word_list, embedding_map, max_queries=100):
        """
        åˆå§‹åŒ–æœç´¢å™¨
        """
        self.faiss_index = faiss_index
        self.word_list = word_list
        self.embedding_map = embedding_map
        self.word_set = set(word_list)
        self.max_queries = max_queries
        self.run_initial_exploration = True # æ§åˆ¶æ˜¯å¦æ‰§è¡Œåˆå§‹æ¢ç´¢

        # ç»´æŠ¤æœç´¢çŠ¶æ€
        self.queried = {}  # å·²æŸ¥è¯¢è¯åŠå…¶ç›¸ä¼¼åº¦ {word: similarity}

        # å–å‰Kä¸ªè¯ä¸­çš„éšæœºä¸€ä¸ªè¿›è¡Œæ–°çš„æœç´¢
        self.search_window_size = 5
        # faissæœç´¢è¿”å›æ•°é‡
        self.faiss_search_size = 50

        # å­˜å‚¨ (ç›¸ä¼¼åº¦, åµŒå…¥å‘é‡)ï¼Œä½¿ç”¨æœ€å°å †ä»¥ä¾¿é«˜æ•ˆè·å–æœ€é«˜å’Œæœ€ä½ç›¸ä¼¼åº¦é¡¹
        self.top_k_heap = []

    def _update_state(self, word, similarity):
        """æ›´æ–°æœç´¢çŠ¶æ€ï¼ŒåŒ…æ‹¬è®°å½•å·²æŸ¥è¯¢è¯å’Œç»´æŠ¤æœ€å¤§å †ã€‚"""
        if word not in self.queried:
            self.queried[word] = similarity
            # åªæœ‰ç›¸ä¼¼åº¦å¤§äº0çš„è¯æ‰å€¼å¾—æ”¾å…¥å †ä¸­ä½œä¸ºæœªæ¥çš„ç§å­
            if similarity > 0:
                heapq.heappush(self.top_k_heap, (-similarity, word))

    def _find_unqueried_neighbors(self, seed_word):
        """ä½¿ç”¨FAISSæŸ¥æ‰¾ä¸€ä¸ªç§å­è¯çš„æœªè¢«æŸ¥è¯¢è¿‡çš„è¿‘é‚»ã€‚"""
        # ç¼–ç ç§å­è¯å¹¶è¿›è¡ŒL2å½’ä¸€åŒ–
        # vector = self.model.encode([seed_word])
        vector = self.embedding_map[seed_word]
        vector = vector.astype('float32').reshape(1, -1)
        faiss.normalize_L2(vector)

        # æ‰©å¤§æœç´¢èŒƒå›´ä»¥è¿‡æ»¤å·²æŸ¥è¯¢çš„è¯
        search_k = self.faiss_search_size + len(self.queried)
        distances, indices = self.faiss_index.search(vector, search_k)

        # è¿‡æ»¤æ‰æ— æ•ˆç´¢å¼•å’Œå·²æŸ¥è¯¢è¿‡çš„è¯
        candidates = [
            self.word_list[i] for i in indices[0]
            if i >= 0 and self.word_list[i] not in self.queried
        ]
        return candidates[:self.faiss_search_size]  # è¿”å›æŒ‡å®šæ•°é‡çš„æœ‰æ•ˆå€™é€‰
        # try:
        #     # ç¼–ç ç§å­è¯å¹¶è¿›è¡ŒL2å½’ä¸€åŒ–
        #     # vector = self.model.encode([seed_word])
        #     vector = self.embedding_map[seed_word]
        #     vector = vector.astype('float32').reshape(1, -1)
        #     faiss.normalize_L2(vector)
        #
        #     # æ‰©å¤§æœç´¢èŒƒå›´ä»¥è¿‡æ»¤å·²æŸ¥è¯¢çš„è¯
        #     search_k = self.faiss_search_size + len(self.queried)
        #     distances, indices = self.faiss_index.search(vector, search_k)
        #
        #     # è¿‡æ»¤æ‰æ— æ•ˆç´¢å¼•å’Œå·²æŸ¥è¯¢è¿‡çš„è¯
        #     candidates = [
        #         self.word_list[i] for i in indices[0]
        #         if i >= 0 and self.word_list[i] not in self.queried
        #     ]
        #     return candidates[:self.faiss_search_size]  # è¿”å›æŒ‡å®šæ•°é‡çš„æœ‰æ•ˆå€™é€‰
        # except Exception as e:
        #     print(f"FAISSæœç´¢æœŸé—´å‘ç”Ÿé”™è¯¯: {e}")
        #     return []

    def _get_next_candidate(self):
        """æ ¹æ®ç­–ç•¥ç”Ÿæˆä¸‹ä¸€ä¸ªè¦æŸ¥è¯¢çš„å€™é€‰è¯ã€‚"""
        # 1. å¦‚æœå †ä¸ºç©ºï¼ˆåˆå§‹é˜¶æ®µæˆ–æ‰€æœ‰ç§å­éƒ½è€—å°½ï¼‰ï¼Œåˆ™ä»æ•´ä¸ªè¯åº“ä¸­éšæœºé€‰ä¸€ä¸ª
        if not self.top_k_heap:
            # ä»æœªæŸ¥è¯¢è¿‡çš„è¯ä¸­éšæœºé€‰æ‹©ä¸€ä¸ª
            unqueried_words = list(self.word_set - set(self.queried.keys()))
            if not unqueried_words: return None  # æ‰€æœ‰è¯éƒ½æŸ¥å®Œäº†
            return random.choice(unqueried_words)

        # 2. ä»å †ä¸­è·å–Top-Kç›¸ä¼¼åº¦çš„è¯ä½œä¸ºç§å­æ± 
        top_k_seeds = heapq.nsmallest(self.search_window_size, self.top_k_heap)

        # 3. éšæœºé€‰æ‹©ä¸€ä¸ªç§å­å¹¶å¯»æ‰¾å…¶è¿‘é‚»
        random.shuffle(top_k_seeds)  # æ‰“ä¹±ç§å­ä»¥å¢åŠ éšæœºæ€§
        for neg_sim, seed_word in top_k_seeds:
            neighbors = self._find_unqueried_neighbors(seed_word)
            if neighbors:
                # ç­–ç•¥ï¼šé€‰æ‹©æœ€è¿‘çš„é‚£ä¸ªæœªè¢«æŸ¥è¯¢è¿‡çš„é‚»å±…
                return random.choice(neighbors[:3])

        # 4. å¦‚æœTop-Kç§å­çš„æ‰€æœ‰è¿‘é‚»éƒ½å·²è¢«æŸ¥è¯¢ï¼Œåˆ™å›é€€åˆ°éšæœºé€‰æ‹©ç­–ç•¥
        unqueried_words = list(self.word_set - set(self.queried.keys()))
        if not unqueried_words: return None
        return random.choice(unqueried_words)

    def run(self, judge):
        """æ‰§è¡Œå®Œæ•´çš„æœç´¢æµç¨‹ã€‚"""
        progress_bar = tqdm(total=self.max_queries, desc="æœç´¢è¿›åº¦", unit="æ¬¡æŸ¥è¯¢")
        best_word, best_similarity = "", 0.0

        for i in range(self.max_queries):
            # è·å–ä¸‹ä¸€ä¸ªå€™é€‰è¯
            candidate = self._get_next_candidate()
            if candidate is None:
                print("æ‰€æœ‰è¯æ±‡å·²æŸ¥è¯¢å®Œæ¯•ã€‚")
                break

            # æŸ¥è¯¢
            similarity = judge.query(candidate)

            # æ›´æ–°çŠ¶æ€
            self._update_state(candidate, similarity)

            # æ›´æ–°æœ€ä½³ç»“æœ
            if similarity > best_similarity:
                best_similarity = similarity
                best_word = candidate

            # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
            progress_bar.update(1)
            progress_bar.set_postfix_str(
                f"æœ€ä½³: '{best_word}' ({best_similarity:.4f}) | å½“å‰: '{candidate}' ({similarity:.4f})")

            # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
            if similarity == 1.0:
                print(f"\nğŸ‰ æˆåŠŸï¼åœ¨ç¬¬ {i + 1} æ¬¡æŸ¥è¯¢æ—¶æ‰¾åˆ°ç›®æ ‡è¯ï¼")
                progress_bar.total = i + 1  # å°†è¿›åº¦æ¡æ€»æ•°è®¾ä¸ºå½“å‰æ¬¡æ•°
                progress_bar.refresh()
                break

        progress_bar.close()
        self._show_final_result()

    def _show_final_result(self):
        """æ˜¾ç¤ºæœ€ç»ˆçš„æœç´¢ç»“æœç»Ÿè®¡ã€‚"""
        if not self.queried:
            print("\næœç´¢ç»“æŸï¼Œæ— ä»»ä½•æŸ¥è¯¢è®°å½•ã€‚")
            return

        # ä»è®°å½•ä¸­æ‰¾åˆ°æœ€ç»ˆçš„æœ€ä½³åŒ¹é…
        final_best_word = max(self.queried, key=self.queried.get)
        final_best_similarity = self.queried[final_best_word]

        print(f"\nğŸ” æœç´¢å®Œæˆã€‚")
        print(f"æœ€ç»ˆç»“æœ: æœ€ä½³åŒ¹é…è¯ '{final_best_word}' (ç›¸ä¼¼åº¦: {final_best_similarity:.4f})")
        print(f"æ€»æŸ¥è¯¢æ¬¡æ•°: {len(self.queried)}")

        # æ˜¾ç¤ºTop-10ç»“æœ
        sorted_items = sorted(self.queried.items(), key=lambda x: x[1], reverse=True)
        print("\nğŸ“Š ç›¸ä¼¼åº¦TOP 10è®°å½•:")
        for i, (word, sim) in enumerate(sorted_items[:10], 1):
            print(f"  {i:2d}. {word:<15s} | ç›¸ä¼¼åº¦: {sim:.4f}")


# ä¸»ç¨‹åº
if __name__ == "__main__":
    # åŠ è½½FAISSç´¢å¼•å’Œè¯æ±‡è¡¨
    try:
        print("æ­£åœ¨åŠ è½½ FAISS ç´¢å¼•å’Œè¯æ±‡è¡¨...")
        import os
        data_path = "data/tight"
        faiss_path = os.path.join(data_path, "vocal_list.faiss")
        vocab_list_path = os.path.join(data_path, "vocal_list.pkl")
        vocab_embedding_path = os.path.join(data_path, "word_embedding_map.pkl")

        if not os.path.exists(faiss_path):
            print(f"Error: {faiss_path} not found.")
            print("è¯·ç¡®ä¿ 'data' æ–‡ä»¶å¤¹å­˜åœ¨ä¸”åŒ…å«é¢„è®¡ç®—çš„ FAISS ç´¢å¼•æ–‡ä»¶ã€‚æ‚¨å¯èƒ½éœ€è¦å…ˆè¿è¡Œä¸€ä¸ªæ•°æ®å‡†å¤‡è„šæœ¬æ¥ç”Ÿæˆè¿™äº›æ–‡ä»¶ã€‚é€€å‡ºã€‚")
            exit()
        if not os.path.exists(vocab_list_path):
            print(f"Error: {vocab_list_path} not found.")
            print("è¯·ç¡®ä¿ 'data' æ–‡ä»¶å¤¹å­˜åœ¨ä¸”åŒ…å«è¯æ±‡åˆ—è¡¨æ–‡ä»¶ã€‚é€€å‡ºã€‚")
            exit()

        faiss_index = faiss.read_index(faiss_path)
        with open(vocab_list_path, "rb") as f:
            word_list = pickle.load(f)

        with open(vocab_embedding_path, "rb") as f:
            embedding_map = pickle.load(f)

    except Exception as e:
        print(f"Error loading data files: {e}")
        print("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæ–‡ä»¶æ ¼å¼ã€‚é€€å‡ºã€‚")
        exit()


    # åˆå§‹åŒ–åˆ¤é¢˜å™¨ï¼ˆè®¾ç½®ç›®æ ‡è¯ï¼‰
    use_local_judge = False
    if use_local_judge:
        # å…±äº«èµ„æºåŠ è½½
        try:
            print("æ­£åœ¨åŠ è½½ SentenceTransformer æ¨¡å‹...")
            # model_name = 'paraphrase-multilingual-MiniLM-L12-v2'
            model_name = 'shibing624/text2vec-base-chinese'
            try:
                model = SentenceTransformer(model_name)
            except Exception as e:
                print(f"Error loading local model '{model_name}': {e}")
                print(f"Attempting to download and load model '{model_name}'...")
                try:
                    model = SentenceTransformer(model_name)  # Re-attempt download
                    print("Model downloaded and loaded successfully.")
                except Exception as download_e:
                    print(f"Error downloading/loading model '{model_name}': {download_e}")
                    print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ¨¡å‹åç§°ã€‚é€€å‡ºã€‚")
                    exit()

            print("æ¨¡å‹åŠ è½½å®Œæˆã€‚")
        except Exception as e:
            print(f"Unexpected error during model loading: {e}")
            print("è¯·æ£€æŸ¥ SentenceTransformer åº“å®‰è£… (pip install sentence-transformers)ã€‚é€€å‡ºã€‚")
            exit()

        judge = LocalJudge("åˆ‘è­¦", model, word_list) # ä½¿ç”¨æœ¬åœ°åˆ¤é¢˜å™¨è°ƒè¯•
    else:
        judge = OnlineJudge() # ä½¿ç”¨åœ¨çº¿åˆ¤é¢˜å™¨

    # åˆå§‹åŒ–æœç´¢å™¨
    # max_queries: æ€»æŸ¥è¯¢æ¬¡æ•°ä¸Šé™
    # run_initial_exploration: æ˜¯å¦æ‰§è¡Œåˆå§‹æ¢ç´¢
    searcher = Searcher(
        faiss_index=faiss_index,
        word_list=word_list,
        embedding_map=embedding_map,
        max_queries=500, # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æ€»æŸ¥è¯¢æ¬¡æ•°
    )

    # æ‰§è¡Œæœç´¢
    searcher.run(judge)